name: Test External Providers

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test-external-providers:
    runs-on: ubuntu-latest
    env:
      INFERENCE_MODEL: llama3.2:3b-instruct-fp16
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install uv
        uses: astral-sh/setup-uv@c7f87aa956e4c323abf06d5dec078e358f6b4d04 # v6.0.0
        with:
          python-version: "3.10"

      - name: Set Up Environment and Install Dependencies
        run: |
          uv sync
          uv pip install -e .

      - name: Cache Ramalama store
        id: ramalama-store-cache
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
        with:
          path: ~/.local/share/ramalama
          key: ramalama-store-${{ env.INFERENCE_MODEL }}

      - name: Download model to serve with Ramalama
        if: ${{ steps.ramalama-store-cache.outputs.cache-hit != 'true' }}
        run: |
          source .venv/bin/activate
          ramalama pull ${{ env.INFERENCE_MODEL }}

      - name: Start Ramalama server in background
        run: |
          # Start ramalama serve in background with logging
          nohup uv run ramalama serve ${{ env.INFERENCE_MODEL }} > ramalama.log 2>&1 &
          RAMALAMA_PID=$!
          echo "Started Ramalama with PID: $RAMALAMA_PID"

          # Wait for ramalama to be ready by testing a chat completion
          echo "Waiting for Ramalama server..."
          for i in {1..60}; do
            echo "Attempt $i to connect to Ramalama..."
            if curl -s -X POST http://localhost:8080/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{"messages": [{"role": "user", "content": "Hello"}], "model": "${{ env.INFERENCE_MODEL }}"}' \
              | grep -q "choices"; then
              echo "Ramalama server is up and responding!"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "Ramalama server failed to start or respond"
              echo "Ramalama logs:"
              cat ramalama.log
              exit 1
            fi
            sleep 1
          done

      - name: Start Llama Stack server in background
        run: |
          source .venv/bin/activate
          touch server.log
          LLAMA_STACK_LOG_FILE=server.log nohup uv run llama stack run run.yaml --image-type venv &

      - name: Wait for Llama Stack server to be ready
        run: |
          echo "Waiting for Llama Stack server..."
          for i in {1..60}; do
            echo "Attempt $i to connect to Llama Stack..."
            if curl -s http://localhost:8321/v1/health | grep -q "OK"; then
              echo "Llama Stack server is up!"
              if grep -q -e "remote::ramalama from .*providers.d/remote/inference/ramalama.yaml" server.log; then
                echo "Llama Stack server is using Ramalama provider"
                exit 0
              else
                echo "Llama Stack server is not using Ramalama provider"
                echo "Server logs:"
                cat server.log
                exit 1
              fi
            fi
            sleep 1
          done
          echo "Llama Stack server failed to start"
          echo "Server logs:"
          cat server.log
          exit 1

      - name: Upload logs
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: logs
          retention-days: 5
          path: |
            **/*.log
