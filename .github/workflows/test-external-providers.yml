name: Test External Providers

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test-external-providers:
    runs-on: ubuntu-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@c6295a65d1254861815972266d5933fd6e532bdf # v2.11.1
        with:
          egress-policy: audit

      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install uv
        uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5.4.2
        with:
          python-version: "3.10"

      - name: Set Up Environment and Install Dependencies
        run: |
          uv sync
          uv pip install -e .

      - name: Start Ramalama server in background
        run: |
          source .venv/bin/activate
          ramalama pull llama3.2:3b-instruct-fp16
          # Start ramalama serve in background with logging
          nohup uv run ramalama serve llama3.2:3b-instruct-fp16 > ramalama.log 2>&1 &
          RAMALAMA_PID=$!
          echo "Started Ramalama with PID: $RAMALAMA_PID"

          # Wait for ramalama to be ready by testing a chat completion
          echo "Waiting for Ramalama server..."
          for i in {1..30}; do
            echo "Attempt $i to connect to Ramalama..."
            if curl -s -X POST http://localhost:8080/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{"messages": [{"role": "user", "content": "Hello"}], "model": "llama3.2:3b-instruct-fp16"}' \
              | grep -q "choices"; then
              echo "Ramalama server is up and responding!"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Ramalama server failed to start or respond"
              echo "Ramalama logs:"
              cat ramalama.log
              exit 1
            fi
            sleep 1
          done

      - name: Start Llama Stack server in background
        run: |
          source .venv/bin/activate
          touch server.log
          INFERENCE_MODEL=llama3.2:3b-instruct-fp16 LLAMA_STACK_LOG_FILE=server.log nohup uv run llama stack run run.yaml --image-type venv &

      - name: Wait for Llama Stack server to be ready
        run: |
          echo "Waiting for Llama Stack server..."
          for i in {1..30}; do
            echo "Attempt $i to connect to Llama Stack..."
            if curl -s http://localhost:8321/v1/health | grep -q "OK"; then
              echo "Llama Stack server is up!"
              if grep -q -e "remote::ramalama from .*providers.d/remote/inference/ramalama.yaml" server.log; then
                echo "Llama Stack server is using Ramalama provider"
                exit 0
              else
                echo "Llama Stack server is not using Ramalama provider"
                echo "Server logs:"
                cat server.log
                exit 1
              fi
            fi
            sleep 1
          done
          echo "Llama Stack server failed to start"
          echo "Server logs:"
          cat server.log
          exit 1

      - name: Upload logs
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: logs
          retention-days: 5
          path: |
            **/*.log
